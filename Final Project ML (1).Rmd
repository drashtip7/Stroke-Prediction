

```{r}
#rm(list=ls())  # clear the environment
stro = read.csv("healthcare-dataset-stroke-data.csv")
#View(stro)
summary(stro)
str(stro)
table(stro$stroke)


```


```{r}
head(stro,10)
tail(stro,10)
names(stro)
dim(stro)
stro = subset (stro, select = -id) # Removed id column as it's value is unique for every row.
str(stro)

# Numerical: Age, averageglucose level,bmi.
# Categorical: Gender, evermarried, work Type, Residence type,smoking status, Hypertension, Heart disease, stroke. 
```

```{r}

# for Numeric Data

boxplot(stro$age ~ stro$stroke, col='orange')
t.test(stro$age ~ stro$stroke, alternative="two.sided", data = stro) # p-value < 2.2e-16

stro$bmi= as.numeric(stro$bmi)
boxplot(stro$bmi ~ stro$stroke, col='orange') 
t.test(stro$bmi ~ stro$stroke, alternative="two.sided", data = stro) # p-value = 0.000334

boxplot(stro$avg_glucose_level ~ stro$stroke, col='orange')
t.test(stro$avg_glucose_level ~ stro$stroke, alternative="two.sided", data = stro) # p-value = 2.401e-11



#For Categorical

x= table(stro$stroke,stro$gender)
x
mosaicplot(x,shade = T)
barplot(x)
chisq.test(x)    # p-value = 0.7895


x= table(stro$stroke,stro$ever_married)
x
mosaicplot(x,shade = T)
barplot(x)
chisq.test(x)    # p-value = 1.639e-14


x= table(stro$stroke,stro$work_type)
x
mosaicplot(x,shade = T)
barplot(x)
chisq.test(x)    # p-value = 5.398e-10


x= table(stro$stroke,stro$Residence_type)
x
mosaicplot(x,shade = T)
barplot(x)
chisq.test(x)    # p-value = 0.2983

x= table(stro$stroke,stro$smoking_status)
x
mosaicplot(x,shade = T)
barplot(x)
chisq.test(x)    # p-value = 2.085e-06

x= table(stro$stroke,stro$hypertension)
x
mosaicplot(x,shade = T)
barplot(x)
chisq.test(x)    # p-value < 2.2e-16

x= table(stro$stroke,stro$heart_disease)
x
mosaicplot(x,shade = T)
barplot(x)
chisq.test(x)    # p-value < 2.2e-16

x= table(stro$stroke,stro$heart_disease)
x
mosaicplot(x,shade = T)
barplot(x)
chisq.test(x)    # p-value < 2.2e-16


# Based on p-values we observe gender and residence type are need to be removed. 

stro = subset (stro, select = -Residence_type)
stro = subset (stro, select = -gender)

str(stro)


```

```{r}

# Conversion of all variables to factor.

stro$hypertension= as.factor(stro$hypertension)
stro$heart_disease= as.factor(stro$heart_disease)
stro$ever_married= as.factor(stro$ever_married)
stro$work_type= as.factor(stro$work_type)
stro$smoking_status= as.factor(stro$smoking_status)
stro$stroke = as.factor(stro$stroke)
str(stro)

```




```{r}
set.seed(1)
stro = stro[sample(1:nrow(stro)),]  # Shuffled the rows of data set.

# Train/Test Split

train = stro[1:4088,]
str(train)                 # Split the first 4088 rows for training 
test=stro[4089:5110,]      
str(test)                  #Split the remaining rows for testing 


colSums(is.na(stro)) # bmi variable has 201 missing values.

#sum(stroke$gender=="Other")


# Finding mean from train and substituing the missing values in train and test.

train$bmi[which(is.na(train$bmi))] = 28.10
test$bmi[which(is.na(test$bmi))] = 28.10

colSums(is.na(train))
colSums(is.na(test))

```
```{r}
str(train)

# Conversion of variables to Numeric for Smote.

train$stroke = as.numeric(train$stroke)-1
train$hypertension= as.numeric(train$hypertension)-1
train$heart_disease= as.numeric(train$heart_disease)-1
train$ever_married= as.numeric(train$ever_married)-1
train$work_type= as.numeric(train$work_type)-1
train$smoking_status= as.numeric(train$smoking_status)-1
str(train)

test$stroke = as.numeric(test$stroke)-1
test$hypertension= as.numeric(test$hypertension)-1
test$heart_disease= as.numeric(test$heart_disease)-1
test$ever_married= as.numeric(test$ever_married)-1
test$work_type= as.numeric(test$work_type)-1
test$smoking_status= as.numeric(test$smoking_status)-1
str(test)

```



```{r}
# Data is imbalance, using SMOTE Technique to balance the data.

library(smotefamily)
library(DMwR2)

smote_output = SMOTE(train[,-9], train$stroke)

balanced_data = smote_output$data  # New data frame is created with target variable "class", which has almost balanced data.
str(balanced_data)
balanced_data$class = as.numeric(balanced_data$class)
hist(balanced_data$class, col ="Orange")

prop.table(table(balanced_data$class)) 
```


```{r}
library(gmodels)
library(crosstable)
library(caret)
library(mlbench)

# Applying Logistic Model
logistic_model = glm(class~., data =balanced_data, family = "binomial")
summary(logistic_model)

predictions = predict(logistic_model,test,type = "response")
head(predictions)

#Converting probabilities to actual labels
predicted.labels = factor(ifelse(predictions>0.5, "1","0"))
head(predicted.labels)
CrossTable(test$stroke,predicted.labels)
t= table(test$stroke,predicted.labels)
t

# Creating Confusion Matrix
test$stroke_1 = as.factor(test$stroke)
confusionMatrix(test$stroke_1,predicted.labels)

#Accuracy = TP+TN/total
accuracy_logistic = (t[1,1]+t[2,2])/(t[1,1]+t[1,2]+t[2,1]+t[2,2]) 
print(accuracy_logistic)  # 0.7632094

# Calculate the area under the ROC curve
library(pROC)
roc.curve = roc(predictions, test$stroke, ci=T)

# Plot the ROC curve
plot(roc.curve)

#Precision : TP / (TP+FP)
prec_no = t[1,1]/(t[1,1]+t[2,1])
prec_no
prec_yes = t[2,2]/(t[2,2]+t[1,2])
prec_yes

#Recall : TP / (TP+FN)
reca_no = t[1,1]/(t[1,1]+t[1,2])
reca_yes = t[2,2]/(t[2,2]+t[2,1])
reca_no
reca_yes
```





```{r}

# Applying Logistic Regression with limited variables 
logistic_model_1 = glm(class~age+hypertension+heart_disease+work_type+avg_glucose_level, data =balanced_data, family = "binomial")
summary(logistic_model_1)

predictions_1 = predict(logistic_model_1,test,type = "response")
head(predictions_1)

#Converting probabilities to actual labels
predicted.labels_1 = factor(ifelse(predictions_1 > 0.5, "1","0"))
head(predicted.labels_1)

t1= table(test$stroke,predicted.labels_1)
t1

#Accuracy = TP+TN/total
accuracy_logi = (t1[1,1]+t1[2,2])/(t1[1,1]+t1[1,2]+t1[2,1]+t1[2,2]) 
print(accuracy_logi)  # Better than previous one.  # 0.7651663

# Creating Confusion Matrix
test$stroke_1 = as.factor(test$stroke)
confusionMatrix(test$stroke_1,predicted.labels_1)

#Precision : TP / (TP+FP)
prec_no = t1[1,1]/(t1[1,1]+t1[2,1])
prec_no
prec_yes = t1[2,2]/(t1[2,2]+t1[1,2])
prec_yes

#Recall : TP / (TP+FN)
reca_no = t1[1,1]/(t1[1,1]+t1[1,2])
reca_yes = t1[2,2]/(t1[2,2]+t1[2,1])
reca_no
reca_yes

```

```{r}
# Applying Decision Tree

balanced_data$class = as.factor(balanced_data$class)
test$stroke = as.factor(test$stroke)

library(C50)
stroke_model30 = C5.0(balanced_data[-9],balanced_data$class,trials = 30)
stroke_model30

stroke_pred30 = predict(stroke_model30,test)

library(gmodels)
t2= table(test$stroke,stroke_pred30)
t2


# Creating Confusion Matrix
confusionMatrix(test$stroke_1,stroke_pred30)

test$stroke = as.numeric(test$stroke)
library(pROC)
roc.curve = roc(stroke_pred30, test$stroke, ci=T)

# Plot the ROC curve
plot(roc.curve)
accuracy_decision = (t2[1,1]+t2[2,2])/(t2[1,1]+t2[1,2]+t2[2,1]+t2[2,2]) 
accuracy_decision  #0.9412916

#Precision : TP / (TP+FP)
prec_no = t2[1,1]/(t2[1,1]+t2[2,1])
prec_no
prec_yes = t2[2,2]/(t2[2,2]+t2[1,2])
prec_yes


#Recall : TP / (TP+FN)
reca_no = t2[1,1]/(t2[1,1]+t2[1,2])
reca_yes = t2[2,2]/(t2[2,2]+t2[2,1])
reca_no
reca_yes
```



```{r}
set.seed(1)
library(randomForest)
library(caret)

ctrl = trainControl(method = "cv", number = 10)
grid_rf = expand.grid(mtry = c(2, 4, 8, 16))

balanced_data$class = as.factor(balanced_data$class) 
m_rf = train(class ~ ., data = balanced_data, method = "rf", trControl = ctrl, tuneGrid = grid_rf)

m_rf

rf_predictions_binary = predict(m_rf, test)
t3 = table(rf_predictions_binary, test$stroke)
t3

# Creating Confusion Matrix
confusionMatrix(test$stroke_1,rf_predictions_binary)

accuracy_random_forest = (t3[1,1]+t3[2,2])/(t3[1,1]+t3[1,2]+t3[2,1]+t3[2,2]) 
accuracy_random_forest  #0.9422701

# Calculate the area under the ROC curve
library(pROC)
roc.curve = roc(rf_predictions_binary, test$stroke, ci=T)

# Plot the ROC curve
plot(roc.curve)

varImp(m_rf)

#Precision : TP / (TP+FP)
prec_no = t3[1,1]/(t3[1,1]+t3[2,1])
prec_no
prec_yes = t3[2,2]/(t3[2,2]+t3[1,2])
prec_yes


#Recall : TP / (TP+FN)
reca_no = t3[1,1]/(t3[1,1]+t3[1,2])
reca_yes = t3[2,2]/(t3[2,2]+t3[2,1])
reca_no
reca_yes

```

```{r}
#              Logistic Regression     Decision Tree    Random forest
#Accuracy :        0.7632094               0.9412916         0.9422701
#Kappa:            0.1652                  0.0092            0.0109

```

